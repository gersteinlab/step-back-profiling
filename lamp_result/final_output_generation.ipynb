{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1688785859119
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Power of Symmetry in Object Recognition: A Computational Framework for Recovering and Grouping Parts in Cluttered Scenes.\n",
            "Improved Range Image Registration through Segmentation-Based Correspondence Matching\n",
            "Efficient Exploitation of Linear Channel Codes for Asymmetric Lossless Distributed Source Coding Using Syndrome Formers and Inverse Syndrome Formers\n",
            "Efficient and Secure Password-Authenticated Group Diffie-Hellman Key Exchange for N-Party Setting\n",
            "Improving the Gallant-Lambert-Vanstone Method for Speeding Up Scalar Multiplication on Elliptic Curves\n",
            "Advancements in 3D Mesh Data: Novel Techniques for Reliable Surface Registration of Non-Rigid and Articulated Objects.\n",
            "\"Enhancing Perceptual Accuracy in Dynamic Textured Sequences through Temporal Modeling and Maximum Entropy Spectral Analysis\"\n",
            "Perspective Skewed Mirror Symmetry and Invariance Research\n",
            "Quantum-Inspired Particle Swarm Optimization for String Pattern Recognition with Evolving Spiking Neural Networks\n",
            "Exploring the Richness of Human Verbal and Nonverbal Expressions: Insights from Basic Science and Technology Applications.\n",
            "Quantitative Analysis of Vocal Entrainment in Conflictual Marital Interactions: Implications for Couple Therapy Outcomes\n",
            "Proactive Problem Detection and Management in Database Management Systems\n",
            "Exploring the Topological Properties and Enumeration of Polypentagons: From Catacondensed Systems to Proper Polypentagons\n",
            "Atlas-based Segmentation of Deep Brain Structures using Spatial Dependency Tree and Non-rigid Registration\n",
            "Modular Architecture for ECG Beat Classification with Batch Modular Learning\n",
            "New Constructions of Re-splittable Threshold Public Key Encryption Schemes Based on Discrete Logarithm-Type Assumptions\n",
            "Improving File-System Performance over Flash Memory with Filter-Driver-Layered Caching Design\n",
            "Efficient Online Hot-Data Identification for Flash-Memory Storage Systems\n",
            "Localizing and Quantifying Inter-Domain Congestion in the Internet Using Time Sequence Latency Probes (TSLP)\n",
            "Controlling Power-Supply Noise for Accurate Path-Sensitization and Test-Pattern Generation\n",
            "Symbolic Counterexample Generation for Discrete-Time Markov Chains\n",
            "Curve-based Dewarping Technique for Document Images Captured with Digital Cameras\n",
            "Breaking Masked AES Implementations with Side-Channel Collision Attacks\n",
            "Sicherheitsrisiken von Funktüröffnersystemen: Erfolgreicher Seitenkanalangriff auf automatisierte Türöffnungssysteme\n",
            "P4R: A Lightweight Cryptographic Payment Scheme for Transit Systems with Refunds\n",
            "A Digital Image Processor Based on Simplicial CNN Cell for High-Speed and High-Resolution Image Processing Systems\n",
            "Exploring Dynamic Value-Added Propositions in Service-Oriented Architecture with Web Services\n",
            "The Surprising Complexity of Translated Convex Bodies in 3D Space\n",
            "Expected Size of 2D Visibility Complex for Randomly Distributed Objects in the Plane\n",
            "Interactive Visual Query Processing for Content-Based Image Retrieval Using a Modified Self-Organizing Map\n",
            "Enhancing Virtual Navigation with Snapping-to-Photos Interfaces in 3D Reconstructed Scenes\n",
            "Improving Precision Position Control of X-Y Table in CNC Machining Centers Using Friction Compensation at Velocity Reversal\n",
            "Supervisory Control Theory for Logic Control Synthesis in Manufacturing Systems: An Educational Test-Bed Using LEGO® Blocks\n",
            "Uncovering the Secrets of High-Dimensional Data Visualization: The Importance of Sensitivity and Plasticity in Dimensionality Reduction Methods\n",
            "Relaxing Assumptions in Multi-Periodic Inventory Control: A Hybrid Meta-Heuristic Intelligent Algorithm for Fuzzy Mixed-Integer Nonlinear Programming with Total Discounts and Shortages\n",
            "Secure Delegation of Elliptic-Curve Pairing Computation with Detection of Cheating\n",
            "Timeout 41482\n",
            "Improved Dense Scene Flow Estimation with Rotation and Translation Modeling for 3D Motion\n",
            "Efficient Large-Scale Hyperspectral Image Classification using Improved Spectral Clustering with Nystrom Extension and Anchor-Based Graph\n",
            "Inferring Spatial Location of Twitter Users through Text and Friendship Network Analysis\n",
            "Bayesian Nonparametric Model for Multi-Label Learning with Flexible Label Embedding\n",
            "Efficiently Removing Harmful Templates from Web Pages\n"
          ]
        }
      ],
      "source": [
        "import concurrent.futures\n",
        "import csv\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "from heapq import nlargest\n",
        "\n",
        "import openai\n",
        "import torch\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "TOP_K = 5\n",
        "Starting_id=41445\n",
        "error_list=[]\n",
        "i = 0\n",
        "openai.api_type = \"azure\"\n",
        "openai.api_base = \"https://biocodeeval-openai.openai.azure.com/\"\n",
        "openai.api_version = \"2023-05-15\"\n",
        "openai.api_key = '' # get this API key from the resource (its not inside the OpenAI deployment portal)\n",
        "\n",
        "key_bundles = [\n",
        "    ('', \"https://biocodeeval-openai.openai.azure.com/\"),\n",
        "    ('', \"https://biocodeeval-openai2.openai.azure.com/\"),\n",
        "    ('', \"https://biocodeeval-openai3.openai.azure.com/\")\n",
        "]\n",
        "\n",
        "# Contriever part\n",
        "tokenizer = AutoTokenizer.from_pretrained('facebook/contriever')\n",
        "model = AutoModel.from_pretrained('facebook/contriever')\n",
        "\n",
        "# Mean pooling\n",
        "def mean_pooling(token_embeddings, mask):\n",
        "    token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n",
        "    sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\n",
        "    return sentence_embeddings\n",
        "\n",
        "# prompt_construction\n",
        "def construct_prompt(question, evidences):\n",
        "    prompt_str = \"If the user have browse the following scholar papers with such titles.\\n{evidences}\\n Please Generate a title for the following abstract of a paper so that the user may be interested: {question}.\\Title:\"\n",
        "    evidences_str = \"\\n\".join(evidences)\n",
        "    prompt_str = prompt_str.replace(\"{evidences}\",evidences_str)\n",
        "    prompt_str = prompt_str.replace(\"{question}\",question)\n",
        "    return prompt_str\n",
        "    \n",
        "with open('dev_questions.json', 'r', encoding='utf-8') as f:\n",
        "    jsonObject = json.load(f)\n",
        "f.close()\n",
        "input_dict = {}\n",
        "profile_dict = {}\n",
        "for item in jsonObject:\n",
        "    input_dict[item[\"id\"]] = item[\"input\"]\n",
        "    input_str = input_dict[item[\"id\"]].replace(\"Generate a title for the following abstract of a paper: \",\"\")\n",
        "    profile_dict[item[\"id\"]] = [input_str]\n",
        "    for profile in item[\"profile\"]:\n",
        "        profile_dict[item[\"id\"]].append(\"Abstract: \"+profile[\"abstract\"]+\"\\nTitle: \" + profile[\"title\"])\n",
        "\n",
        "with open(\"naive_prompt_task5_3.tsv\", \"wb\", buffering=0) as out_file:\n",
        "    with torch.no_grad():\n",
        "        for indice in profile_dict:\n",
        "            if float(indice) <= Starting_id:\n",
        "                continue\n",
        "            doc_chunks = []\n",
        "            # Apply tokenizer\n",
        "            inputs = tokenizer(profile_dict[indice], padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "            # Compute token embeddings\n",
        "            outputs = model(**inputs)\n",
        "            embeddings = mean_pooling(outputs[0], inputs['attention_mask'])\n",
        "            score_index = []\n",
        "            for i in range(1,len(profile_dict[indice])):\n",
        "                score_index.append({\"ordinal\":i,\"score\":float((embeddings[0] @ embeddings[i]).cpu().detach())})\n",
        "            top_score_index = nlargest(TOP_K, score_index, key=lambda item: item[\"score\"])\n",
        "            top_score_docs = [profile_dict[indice][i] for i in (i[\"ordinal\"] for i in top_score_index)]\n",
        "            prompt = construct_prompt(profile_dict[indice][0],top_score_docs)\n",
        "            my_dict = {}\n",
        "            my_dict[\"role\"] = \"user\"\n",
        "            my_dict[\"content\"] = prompt\n",
        "            l = []\n",
        "            l.append(my_dict)\n",
        "            try:\n",
        "                key_bundle = key_bundles[i%3]\n",
        "                openai.api_key, openai.api_base = key_bundle\n",
        "                response = openai.ChatCompletion.create(engine=\"gpt-35-turbo\", messages=l, temperature=0.0,request_timeout=30)\n",
        "                result = response.choices[0].message[\"content\"].replace('\\n', ' ')\n",
        "                write_str = bytes(indice+\"\\t\"+result+\"\\n\", 'utf-8')\n",
        "                out_file.write(write_str)\n",
        "                print(result)\n",
        "                i = i + 1\n",
        "            except openai.error.Timeout:\n",
        "                print(\"Timeout\",indice)\n",
        "                key_bundle = key_bundles[i%3]\n",
        "                openai.api_key, openai.api_base = key_bundle\n",
        "                try:\n",
        "                    response = openai.ChatCompletion.create(engine=\"gpt-35-turbo\", messages=l, temperature=0.0,request_timeout=30)\n",
        "                    result = response.choices[0].message[\"content\"].replace('\\n', ' ')\n",
        "                    write_str = bytes(indice+\"\\t\"+result+\"\\n\", 'utf-8')\n",
        "                    out_file.write(write_str)\n",
        "                    i = i + 1\n",
        "                except openai.error.Timeout:\n",
        "                    print(\"Timeout\",indice)\n",
        "                    key_bundle = key_bundles[i%3]\n",
        "                    openai.api_key, openai.api_base = key_bundle\n",
        "                    response = openai.ChatCompletion.create(engine=\"gpt-35-turbo\", messages=l, temperature=0.0,request_timeout=30)\n",
        "                    result = response.choices[0].message[\"content\"].replace('\\n', ' ')\n",
        "                    write_str = bytes(indice+\"\\t\"+result+\"\\n\", 'utf-8')\n",
        "                    out_file.write(write_str)\n",
        "                except Exception as e:\n",
        "                    print(f\"An error occurred: {str(e)}\")\n",
        "                    error_list.append(i)\n",
        "                #print(result)\n",
        "            except openai.error.InvalidRequestError:\n",
        "                print(\"InvalidRequestError\",indice)\n",
        "                error_list.append(i)\n",
        "                #print(result)\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred: {str(e)}\")\n",
        "                error_list.append(i)\n",
        "            i = i + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1690984278659
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2498"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TASK 4-7 \n",
        "\n",
        "\n",
        "import csv\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "class LazyDecoder(json.JSONDecoder):\n",
        "    def decode(self, s, **kwargs):\n",
        "        regex_replacements = [\n",
        "            (re.compile(r'([^\\\\])\\\\([^\\\\])'), r'\\1\\\\\\\\\\2'),\n",
        "            (re.compile(r',(\\s*])'), r'\\1'),\n",
        "        ]\n",
        "        for regex, replacement in regex_replacements:\n",
        "            s = regex.sub(replacement, s)\n",
        "        return super().decode(s, **kwargs)\n",
        "    \n",
        "task_num = \"4\"\n",
        "prompt_version = \"1.67\"\n",
        "example_num = \"30\"\n",
        "\n",
        "id2result = {}\n",
        "id2input = {}\n",
        "\n",
        "\n",
        "results = {\n",
        "    \"task\": \"LaMP_\"+task_num,\n",
        "    \"golds\":[]\n",
        "}\n",
        "result_df = pd.read_csv('cot_prompt_agg_reasoning_task5_5_examples_prompt_random_v1.67_test.tsv', sep='\\t',header=None, names=[\"id\",\"output\",\"prompt\"])\n",
        "for i in range(len(result_df[\"id\"])):\n",
        "    output_result = result_df[\"output\"][i]\n",
        "    results[\"golds\"].append({\"id\":str(result_df[\"id\"][i]),\"output\":output_result.replace(\"\\\"\",\"\").replace(\",\",\"\")})\n",
        "\n",
        "with open('task5_result_user_based.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "with open('task5_result_user_based.json', 'r') as f:\n",
        "    s1 = json.load(f, cls=LazyDecoder)\n",
        "\n",
        "len(results[\"golds\"])"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
