{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\r\n",
        "import csv\r\n",
        "import json\r\n",
        "import logging\r\n",
        "import os\r\n",
        "import random\r\n",
        "import sys\r\n",
        "from heapq import nlargest\r\n",
        "\r\n",
        "import openai\r\n",
        "import torch\r\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\r\n",
        "from transformers import AutoModel, AutoTokenizer\r\n",
        "\r\n",
        "TOP_K = 5\r\n",
        "Starting_id=41445\r\n",
        "error_list=[]\r\n",
        "i = 0\r\n",
        "openai.api_type = \"azure\"\r\n",
        "openai.api_base = \"https://biocodeeval-openai.openai.azure.com/\"\r\n",
        "openai.api_version = \"2023-05-15\"\r\n",
        "openai.api_key = 'aaccba8e27374383beb397ecdc615ee5' # get this API key from the resource (its not inside the OpenAI deployment portal)\r\n",
        "\r\n",
        "key_bundles = [\r\n",
        "    ('aaccba8e27374383beb397ecdc615ee5', \"https://biocodeeval-openai.openai.azure.com/\"),\r\n",
        "    ('3a648cbe477c4c0c8061cbdd0a4b8855', \"https://biocodeeval-openai2.openai.azure.com/\"),\r\n",
        "    ('7864e774f3db4066a54c1979672f316c', \"https://biocodeeval-openai3.openai.azure.com/\")\r\n",
        "]\r\n",
        "\r\n",
        "# Contriever part\r\n",
        "tokenizer = AutoTokenizer.from_pretrained('facebook/contriever')\r\n",
        "model = AutoModel.from_pretrained('facebook/contriever')\r\n",
        "\r\n",
        "# Mean pooling\r\n",
        "def mean_pooling(token_embeddings, mask):\r\n",
        "    token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\r\n",
        "    sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\r\n",
        "    return sentence_embeddings\r\n",
        "\r\n",
        "# prompt_construction\r\n",
        "def construct_prompt(question, evidences):\r\n",
        "    prompt_str = \"If the user have browse the following scholar papers with such titles.\\n{evidences}\\n Please Generate a title for the following abstract of a paper so that the user may be interested: {question}.\\Title:\"\r\n",
        "    evidences_str = \"\\n\".join(evidences)\r\n",
        "    prompt_str = prompt_str.replace(\"{evidences}\",evidences_str)\r\n",
        "    prompt_str = prompt_str.replace(\"{question}\",question)\r\n",
        "    return prompt_str\r\n",
        "    \r\n",
        "with open('dev_questions.json', 'r', encoding='utf-8') as f:\r\n",
        "    jsonObject = json.load(f)\r\n",
        "f.close()\r\n",
        "input_dict = {}\r\n",
        "profile_dict = {}\r\n",
        "for item in jsonObject:\r\n",
        "    input_dict[item[\"id\"]] = item[\"input\"]\r\n",
        "    input_str = input_dict[item[\"id\"]].replace(\"Generate a title for the following abstract of a paper: \",\"\")\r\n",
        "    profile_dict[item[\"id\"]] = [input_str]\r\n",
        "    for profile in item[\"profile\"]:\r\n",
        "        profile_dict[item[\"id\"]].append(\"Abstract: \"+profile[\"abstract\"]+\"\\nTitle: \" + profile[\"title\"])\r\n",
        "\r\n",
        "with open(\"naive_prompt_task5_3.tsv\", \"wb\", buffering=0) as out_file:\r\n",
        "    with torch.no_grad():\r\n",
        "        for indice in profile_dict:\r\n",
        "            if float(indice) <= Starting_id:\r\n",
        "                continue\r\n",
        "            doc_chunks = []\r\n",
        "            # Apply tokenizer\r\n",
        "            inputs = tokenizer(profile_dict[indice], padding=True, truncation=True, return_tensors='pt')\r\n",
        "\r\n",
        "            # Compute token embeddings\r\n",
        "            outputs = model(**inputs)\r\n",
        "            embeddings = mean_pooling(outputs[0], inputs['attention_mask'])\r\n",
        "            score_index = []\r\n",
        "            for i in range(1,len(profile_dict[indice])):\r\n",
        "                score_index.append({\"ordinal\":i,\"score\":float((embeddings[0] @ embeddings[i]).cpu().detach())})\r\n",
        "            top_score_index = nlargest(TOP_K, score_index, key=lambda item: item[\"score\"])\r\n",
        "            top_score_docs = [profile_dict[indice][i] for i in (i[\"ordinal\"] for i in top_score_index)]\r\n",
        "            prompt = construct_prompt(profile_dict[indice][0],top_score_docs)\r\n",
        "            my_dict = {}\r\n",
        "            my_dict[\"role\"] = \"user\"\r\n",
        "            my_dict[\"content\"] = prompt\r\n",
        "            l = []\r\n",
        "            l.append(my_dict)\r\n",
        "            try:\r\n",
        "                key_bundle = key_bundles[i%3]\r\n",
        "                openai.api_key, openai.api_base = key_bundle\r\n",
        "                response = openai.ChatCompletion.create(engine=\"gpt-35-turbo\", messages=l, temperature=0.0,request_timeout=30)\r\n",
        "                result = response.choices[0].message[\"content\"].replace('\\n', ' ')\r\n",
        "                write_str = bytes(indice+\"\\t\"+result+\"\\n\", 'utf-8')\r\n",
        "                out_file.write(write_str)\r\n",
        "                print(result)\r\n",
        "                i = i + 1\r\n",
        "            except openai.error.Timeout:\r\n",
        "                print(\"Timeout\",indice)\r\n",
        "                key_bundle = key_bundles[i%3]\r\n",
        "                openai.api_key, openai.api_base = key_bundle\r\n",
        "                try:\r\n",
        "                    response = openai.ChatCompletion.create(engine=\"gpt-35-turbo\", messages=l, temperature=0.0,request_timeout=30)\r\n",
        "                    result = response.choices[0].message[\"content\"].replace('\\n', ' ')\r\n",
        "                    write_str = bytes(indice+\"\\t\"+result+\"\\n\", 'utf-8')\r\n",
        "                    out_file.write(write_str)\r\n",
        "                    i = i + 1\r\n",
        "                except openai.error.Timeout:\r\n",
        "                    print(\"Timeout\",indice)\r\n",
        "                    key_bundle = key_bundles[i%3]\r\n",
        "                    openai.api_key, openai.api_base = key_bundle\r\n",
        "                    response = openai.ChatCompletion.create(engine=\"gpt-35-turbo\", messages=l, temperature=0.0,request_timeout=30)\r\n",
        "                    result = response.choices[0].message[\"content\"].replace('\\n', ' ')\r\n",
        "                    write_str = bytes(indice+\"\\t\"+result+\"\\n\", 'utf-8')\r\n",
        "                    out_file.write(write_str)\r\n",
        "                except Exception as e:\r\n",
        "                    print(f\"An error occurred: {str(e)}\")\r\n",
        "                    error_list.append(i)\r\n",
        "                #print(result)\r\n",
        "            except openai.error.InvalidRequestError:\r\n",
        "                print(\"InvalidRequestError\",indice)\r\n",
        "                error_list.append(i)\r\n",
        "                #print(result)\r\n",
        "            except Exception as e:\r\n",
        "                print(f\"An error occurred: {str(e)}\")\r\n",
        "                error_list.append(i)\r\n",
        "            i = i + 1"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "The Power of Symmetry in Object Recognition: A Computational Framework for Recovering and Grouping Parts in Cluttered Scenes.\nImproved Range Image Registration through Segmentation-Based Correspondence Matching\nEfficient Exploitation of Linear Channel Codes for Asymmetric Lossless Distributed Source Coding Using Syndrome Formers and Inverse Syndrome Formers\nEfficient and Secure Password-Authenticated Group Diffie-Hellman Key Exchange for N-Party Setting\nImproving the Gallant-Lambert-Vanstone Method for Speeding Up Scalar Multiplication on Elliptic Curves\nAdvancements in 3D Mesh Data: Novel Techniques for Reliable Surface Registration of Non-Rigid and Articulated Objects.\n\"Enhancing Perceptual Accuracy in Dynamic Textured Sequences through Temporal Modeling and Maximum Entropy Spectral Analysis\"\nPerspective Skewed Mirror Symmetry and Invariance Research\nQuantum-Inspired Particle Swarm Optimization for String Pattern Recognition with Evolving Spiking Neural Networks\nExploring the Richness of Human Verbal and Nonverbal Expressions: Insights from Basic Science and Technology Applications.\nQuantitative Analysis of Vocal Entrainment in Conflictual Marital Interactions: Implications for Couple Therapy Outcomes\nProactive Problem Detection and Management in Database Management Systems\nExploring the Topological Properties and Enumeration of Polypentagons: From Catacondensed Systems to Proper Polypentagons\nAtlas-based Segmentation of Deep Brain Structures using Spatial Dependency Tree and Non-rigid Registration\nModular Architecture for ECG Beat Classification with Batch Modular Learning\nNew Constructions of Re-splittable Threshold Public Key Encryption Schemes Based on Discrete Logarithm-Type Assumptions\nImproving File-System Performance over Flash Memory with Filter-Driver-Layered Caching Design\nEfficient Online Hot-Data Identification for Flash-Memory Storage Systems\nLocalizing and Quantifying Inter-Domain Congestion in the Internet Using Time Sequence Latency Probes (TSLP)\nControlling Power-Supply Noise for Accurate Path-Sensitization and Test-Pattern Generation\nSymbolic Counterexample Generation for Discrete-Time Markov Chains\nCurve-based Dewarping Technique for Document Images Captured with Digital Cameras\nBreaking Masked AES Implementations with Side-Channel Collision Attacks\nSicherheitsrisiken von Funktüröffnersystemen: Erfolgreicher Seitenkanalangriff auf automatisierte Türöffnungssysteme\nP4R: A Lightweight Cryptographic Payment Scheme for Transit Systems with Refunds\nA Digital Image Processor Based on Simplicial CNN Cell for High-Speed and High-Resolution Image Processing Systems\nExploring Dynamic Value-Added Propositions in Service-Oriented Architecture with Web Services\nThe Surprising Complexity of Translated Convex Bodies in 3D Space\nExpected Size of 2D Visibility Complex for Randomly Distributed Objects in the Plane\nInteractive Visual Query Processing for Content-Based Image Retrieval Using a Modified Self-Organizing Map\nEnhancing Virtual Navigation with Snapping-to-Photos Interfaces in 3D Reconstructed Scenes\nImproving Precision Position Control of X-Y Table in CNC Machining Centers Using Friction Compensation at Velocity Reversal\nSupervisory Control Theory for Logic Control Synthesis in Manufacturing Systems: An Educational Test-Bed Using LEGO® Blocks\nUncovering the Secrets of High-Dimensional Data Visualization: The Importance of Sensitivity and Plasticity in Dimensionality Reduction Methods\nRelaxing Assumptions in Multi-Periodic Inventory Control: A Hybrid Meta-Heuristic Intelligent Algorithm for Fuzzy Mixed-Integer Nonlinear Programming with Total Discounts and Shortages\nSecure Delegation of Elliptic-Curve Pairing Computation with Detection of Cheating\nTimeout 41482\nImproved Dense Scene Flow Estimation with Rotation and Translation Modeling for 3D Motion\nEfficient Large-Scale Hyperspectral Image Classification using Improved Spectral Clustering with Nystrom Extension and Anchor-Based Graph\nInferring Spatial Location of Twitter Users through Text and Friendship Network Analysis\nBayesian Nonparametric Model for Multi-Label Learning with Flexible Label Embedding\nEfficiently Removing Harmful Templates from Web Pages\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1688785859119
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 4-7 \r\n",
        "\r\n",
        "\r\n",
        "import csv\r\n",
        "import json\r\n",
        "import pandas as pd\r\n",
        "import re\r\n",
        "class LazyDecoder(json.JSONDecoder):\r\n",
        "    def decode(self, s, **kwargs):\r\n",
        "        regex_replacements = [\r\n",
        "            (re.compile(r'([^\\\\])\\\\([^\\\\])'), r'\\1\\\\\\\\\\2'),\r\n",
        "            (re.compile(r',(\\s*])'), r'\\1'),\r\n",
        "        ]\r\n",
        "        for regex, replacement in regex_replacements:\r\n",
        "            s = regex.sub(replacement, s)\r\n",
        "        return super().decode(s, **kwargs)\r\n",
        "    \r\n",
        "task_num = \"4\"\r\n",
        "prompt_version = \"1.67\"\r\n",
        "example_num = \"30\"\r\n",
        "\r\n",
        "id2result = {}\r\n",
        "id2input = {}\r\n",
        "\r\n",
        "# with open('input/task'+task_num+'_dev_questions.json', 'r', encoding='utf-8') as f:\r\n",
        "#     inputs = json.load(f)\r\n",
        "# for i in inputs:\r\n",
        "#     id2input[i[\"id\"]]=i[\"input\"].replace(\"Paraphrase the following tweet without any explanation before or after it: \",\"\")\r\n",
        "\r\n",
        "# with open('task'+task_num+'_dev_outputs.json', 'r', encoding='utf-8') as f:\r\n",
        "#     labels = json.load(f)\r\n",
        "results = {\r\n",
        "    \"task\": \"LaMP_\"+task_num,\r\n",
        "    \"golds\":[]\r\n",
        "}\r\n",
        "result_df = pd.read_csv('cot_prompt_agg_reasoning_task5_5_examples_prompt_random_v1.67_test.tsv', sep='\\t',header=None, names=[\"id\",\"output\",\"prompt\"])\r\n",
        "for i in range(len(result_df[\"id\"])):\r\n",
        "    output_result = result_df[\"output\"][i]\r\n",
        "    results[\"golds\"].append({\"id\":str(result_df[\"id\"][i]),\"output\":output_result.replace(\"\\\"\",\"\").replace(\",\",\"\")})\r\n",
        "\r\n",
        "with open('task5_result_user_based.json', 'w', encoding='utf-8') as f:\r\n",
        "    json.dump(results, f, indent=4)\r\n",
        "with open('task5_result_user_based.json', 'r') as f:\r\n",
        "    s1 = json.load(f, cls=LazyDecoder)\r\n",
        "\r\n",
        "len(results[\"golds\"])\r\n",
        "# with open('task'+task_num+'_dev_outputs_gold.json', 'w', encoding='utf-8') as f:\r\n",
        "#     json.dump(removed_labels, f, indent=4)\r\n",
        "# with open('task'+task_num+'_dev_outputs_gold.json', 'r', encoding='utf-8') as f:\r\n",
        "#     s1 = json.load(f)\r\n",
        "\r\n",
        "    \r\n",
        "# with open('review_data/review_task'+task_num+'_CoT_promptv'+prompt_version+'_'+example_num+'_examples_mixed.json', 'w', encoding='utf-8') as f:\r\n",
        "#     json.dump(results_for_review, f, indent=4)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "2498"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690984278659
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}